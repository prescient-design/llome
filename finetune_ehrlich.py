import gc
import hydra
import json
import logging
import os
import numpy as np
import pandas as pd
import pprint
import s3fs
import sys
import torch
from contextlib import nullcontext
from datasets import Dataset
from finetune_utils import (
    EvaluatorEditPairs,
    EvaluatorPlainPairs,
    get_response_template_edit_pairs,
    get_response_template_plain_pairs,
    formatting_texts_func_edit_pairs,
    formatting_texts_func_plain_pairs,
    wandb_setup,
)
from model_client import ModelClient
from omegaconf import DictConfig, OmegaConf
from seq2seq_sft_trainer import S3Callback, Seq2SeqSFTConfig, Seq2SeqSFTTrainer
from tqdm.rich import tqdm
from trl.commands.cli_utils import init_zero_verbose
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    GenerationConfig,
    set_seed,
)
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR
from transformers.utils import logging as transformers_logging

from trl import (
    DataCollatorForCompletionOnlyLM,
    ModelConfig,
    RichProgressCallback,
    get_peft_config,
    get_quantization_config,
    get_kbit_device_map,
)
from typing import Any, List, Optional

TRL_USE_RICH = os.environ.get("TRL_USE_RICH", False)

if TRL_USE_RICH:
    init_zero_verbose()
    FORMAT = "%(message)s"

    from rich.console import Console
    from rich.logging import RichHandler

logger = logging.getLogger(__name__)

tqdm.pandas()

if TRL_USE_RICH:
    logging.basicConfig(
        format=FORMAT, datefmt="[%X]", handlers=[RichHandler()], level=logging.INFO
    )


@hydra.main(config_path="config/finetune", config_name="pythia-2.8b")
def main(cfg: DictConfig):
    wandb_setup(cfg)
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
        level=cfg.log_level.upper(),
        force=True,
    )

    cfg_dict = OmegaConf.to_container(cfg)
    generation_config = GenerationConfig(**cfg_dict["generation_config"])
    training_args = Seq2SeqSFTConfig(
        **cfg_dict["training_args"],
        generation_config=generation_config,
    )
    assert (
        training_args.include_inputs_for_metrics == True
    )  # required for compute_metrics
    model_config = ModelConfig(**cfg_dict["model_config"])
    logger.info(f"training_args: {training_args}")
    logger.info(f"model_config: {model_config}")
    logger.info(f"generation config: {training_args.generation_config.to_dict()}")

    # Force use our print callback
    if TRL_USE_RICH:
        training_args.disable_tqdm = True
        console = Console()

    ################
    # Model & Tokenizer
    ################
    torch_dtype = (
        model_config.torch_dtype
        if model_config.torch_dtype in ["auto", None]
        else getattr(torch, model_config.torch_dtype)
    )
    quantization_config = get_quantization_config(model_config)
    device_map = get_kbit_device_map() if quantization_config is not None else None
    model_kwargs = dict(
        # revision=model_config.model_revision,
        trust_remote_code=model_config.trust_remote_code,
        attn_implementation=model_config.attn_implementation,
        torch_dtype=torch_dtype,
        use_cache=False,
        device_map=device_map,
        quantization_config=quantization_config,
    )
    logger.info(f"Model initialization args: {pprint.pformat(model_kwargs)}")
    set_seed(training_args.seed)
    if model_config.model_name_or_path.startswith("s3://"):
        # use the ModelClient class since it has utilities for loading models from S3
        model_client = ModelClient(
            model_config.model_name_or_path, logger=logger, **model_kwargs
        )
        model = model_client.model
        tokenizer = model_client.tokenizer
        tokenizer.padding_side = "right"
    elif cfg.train_from_scratch:
        model_cfg_dict = OmegaConf.to_container(cfg.init_model_config)
        config = AutoConfig.from_pretrained(
            model_config.model_name_or_path, **model_cfg_dict
        )
        model = AutoModelForCausalLM.from_config(config)
        tokenizer = AutoTokenizer.from_pretrained(
            model_config.model_name_or_path, use_fast=True
        )
    else:
        model = AutoModelForCausalLM.from_pretrained(
            model_config.model_name_or_path, **model_kwargs
        )
        tokenizer = AutoTokenizer.from_pretrained(
            model_config.model_name_or_path, use_fast=True
        )
    tokenizer.pad_token = tokenizer.eos_token

    ################
    # Dataset
    ################
    raw_df = pd.read_json(cfg.data_fp, orient="records", lines=True)
    raw_datasets = Dataset.from_pandas(raw_df).train_test_split(
        train_size=cfg.train_size, shuffle=True, seed=training_args.seed
    )
    train_dataset = raw_datasets["train"]
    eval_dataset = raw_datasets["test"]

    if cfg.sanity_check:
        logger.info(f"In sanity check mode, will reduce the number of training steps.")
        train_dataset = train_dataset.select(range(min(len(train_dataset), 1000)))
        eval_dataset = eval_dataset.select(range(min(len(eval_dataset), 50)))
        training_args.num_train_epochs = 1
        training_args.save_strategy = "epoch"
        training_args.eval_strategy = "steps"
        training_args.eval_steps = 5
    else:
        if cfg.max_eval_size is not None:
            eval_dataset = eval_dataset.select(
                range(min(cfg.max_eval_size, len(eval_dataset)))
            )
    logger.info(
        f"Training dataset: {len(train_dataset)} examples\nEval dataset: {len(eval_dataset)} examples\n"
    )

    ################
    # Optional rich context managers
    ###############
    init_context = (
        nullcontext()
        if not TRL_USE_RICH
        else console.status("[bold green]Initializing the SFTTrainer...")
    )
    save_context = (
        nullcontext()
        if not TRL_USE_RICH
        else console.status(
            f"[bold green]Training completed! Saving the model to {training_args.output_dir}"
        )
    )

    ################
    # Training
    ################
    with init_context:
        # use transformers logger during the training loop
        transformers_logging.enable_default_handler()
        transformers_logging.enable_propagation()
        transformers_logger = transformers_logging.get_logger("transformers")
        try:
            transformers_logger.setLevel(cfg.log_level.upper())
        except Exception:
            logger.warning(
                f"Could not set transformers logger to level {cfg.log_level}. Keeping defaults..."
            )

        if cfg.format_type == "plain_pairs":
            formatting_fn = formatting_texts_func_plain_pairs
        elif cfg.format_type == "edit_pairs":
            formatting_fn = formatting_texts_func_edit_pairs
        else:
            raise ValueError(f"Unsupported format type: {cfg.format_type}")

        if cfg.train_from_scratch:
            # train new tokenizer
            logging.info(
                f"Training new tokenizer with vocab size {cfg.init_model_config.vocab_size}"
            )
            formatted_inputs = formatting_fn(train_dataset, include_target=True)
            logging.info(
                f"Example of first 5 examples used for training custom tokenizer: {formatted_inputs[:5]}"
            )
            batch_size = 1000
            train_iterator = [
                formatted_inputs[i : i + batch_size]
                for i in np.arange(0, len(formatted_inputs), batch_size)
            ]
            tokenizer = tokenizer.train_new_from_iterator(
                train_iterator, vocab_size=cfg.init_model_config.vocab_size
            )
            tokenizer.save_pretrained(f"{training_args.output_dir}/custom_tokenizer")
            logging.info(
                f"Trained and saved custom tokenizer with {len(tokenizer)} tokens."
            )

        if cfg.format_type == "plain_pairs":
            response_template_ids = get_response_template_plain_pairs(tokenizer)
            compute_metrics = EvaluatorPlainPairs(
                cfg, tokenizer, logger=transformers_logger
            )
        elif cfg.format_type == "edit_pairs":
            response_template_ids = get_response_template_edit_pairs(tokenizer)
            compute_metrics = EvaluatorEditPairs(
                cfg, tokenizer, logger=transformers_logger
            )
        else:
            raise ValueError(f"Unsupported format type: {cfg.format_type}")

        collator = DataCollatorForCompletionOnlyLM(
            response_template_ids, tokenizer=tokenizer
        )
        callbacks = [RichProgressCallback] if TRL_USE_RICH else []
        if cfg.s3_output_dir is not None:
            s3_callback = S3Callback(cfg.s3_output_dir, logger=transformers_logger)
            callbacks.append(s3_callback)
        trainer = Seq2SeqSFTTrainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
            peft_config=get_peft_config(model_config),
            callbacks=callbacks,
            formatting_func=formatting_fn,
            data_collator=collator,
            compute_metrics=compute_metrics,
            max_seq_length=cfg.seq_length,
        )

    trainer.evaluate()

    trainer.train()

    ################################
    # Prediction with final model
    ################################
    predict_output = trainer.predict(trainer.eval_dataset)

    def find_list_in_other_list(
        haystack: List[Any], needle: List[Any]
    ) -> Optional[int]:
        """If needle exists in haystack, return the first index at which it can be found."""
        for i, elem in enumerate(haystack):
            if elem == needle[0] and haystack[i : i + len(needle)] == needle:
                return i
        return None

    output_token_ids = predict_output.predictions
    output_token_ids[output_token_ids == -100] = tokenizer.pad_token_id
    output_pred_rows = []
    for ex, ex_token_ids in zip(eval_dataset, output_token_ids):
        ex_token_ids_list = ex_token_ids.tolist()
        response_template_idx = find_list_in_other_list(
            ex_token_ids_list, response_template_ids
        )
        if response_template_idx is None:
            transformers_logger.warning(
                f"Could not find response template IDs '{response_template_ids}'"
                + f" in generated tokens. Generated tokens:\n{ex_token_ids_list}"
            )
            continue
        gen_input = ex_token_ids_list[
            : response_template_idx + len(response_template_ids)
        ]
        gen_output = ex_token_ids_list[
            response_template_idx + len(response_template_ids) :
        ]
        output_pred_rows.append(
            {
                "generation_input": tokenizer.decode(
                    gen_input,
                    skip_special_tokens=True,
                    clean_up_tokenization_steps=True,
                ),
                "generation_output": tokenizer.decode(
                    gen_output,
                    skip_special_tokens=True,
                    clean_up_tokenization_steps=True,
                ),
                **ex,
            }
        )
    output_pred_rows = pd.DataFrame(output_pred_rows)
    output_pred_rows.to_json(
        os.path.join(training_args.output_dir, "final_predictions.jsonl"),
        orient="records",
        lines=True,
    )

    with save_context:
        trainer.save_model(training_args.output_dir)
        # Now loop through files in the directory and move to S3 (excluding the checkpoint directories)
        if cfg.s3_output_dir is not None:
            if not cfg.s3_output_dir.endswith("/"):
                cfg.s3_output_dir += "/"
            s3 = s3fs.S3FileSystem()
            for fn in os.listdir(training_args.output_dir):
                if fn.startswith(PREFIX_CHECKPOINT_DIR):
                    continue
                fp = os.path.join(training_args.output_dir, fn)
                recursive = os.path.isdir(fp)
                transformers_logger.info(f"Copying {fp} to {cfg.s3_output_dir}...")
                s3.put(fp, cfg.s3_output_dir, recursive=recursive)


if __name__ == "__main__":
    main()
